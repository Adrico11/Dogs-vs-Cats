{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition : Dogs vs Cats Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess our images (collect the VGG16 - FNN outputs) and save their bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "\n",
    "train_data_dir = 'dogs-vs-cats-redux-kernels-edition/train' #'Lab/data/DogCatChallenge/sampleDeep/train'\n",
    "validation_data_dir = 'dogs-vs-cats-redux-kernels-edition/valid' #'Lab/data/DogCatChallenge/sampleDeep/valid'\n",
    "#test_data_dir = 'C:\\Users\\adrie\\OneDrive\\Bureau\\Informatique\\Projets\\CV\\DTY Lab\\dogs-vs-cats\\test1\\test1'\n",
    "\n",
    "# Function that instanciates the convolutional part of the VGG16 pre-trained model on Imagenet and that runs it on our training and validation data\n",
    "\n",
    "def save_bottleneck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build and load the VGG16 network without the fully connected layers\n",
    "    model = VGG16(\n",
    "        include_top=False, #remove the top fully-connected NN\n",
    "        weights=\"imagenet\")\n",
    "\n",
    "    # preparation of the training data\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "        shuffle=True,\n",
    "        seed=42) # our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n",
    "# the predict_generator method returns the output of a model, given\n",
    "# a generator that yields batches of numpy data\n",
    "    \n",
    "    # Generation of the predictions for the input samples from the training data generator and return them as a numpy array that we can saved\n",
    "    bottleneck_features_train = model.predict(generator)  #nb_train_samples#\n",
    "    # save the output as a Numpy array\n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    # preparation of the validation data\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        #batch_size=batch_size,######\n",
    "        class_mode=None,\n",
    "        shuffle=True,\n",
    "        seed=42)\n",
    "    \n",
    "    # Generation of the predictions for the input samples from the validation data generator and return them as a numpy array that we can saved\n",
    "    bottleneck_features_validation = model.predict(generator) #nb_validation_samples#\n",
    "    np.save(open('bottleneck_features_validation.npy', 'wb'),\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "    \n",
    "# Function that trains a small fully-connected model on top of the stored previous features\n",
    "\n",
    "save_bottleneck_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our added FNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 - 4s - loss: 1.6056 - accuracy: 0.5906 - val_loss: 1.2555 - val_accuracy: 0.5336\n",
      "Epoch 2/50\n",
      "20/20 - 2s - loss: 0.6146 - accuracy: 0.6938 - val_loss: 0.5976 - val_accuracy: 0.6914\n",
      "Epoch 3/50\n",
      "20/20 - 3s - loss: 0.4957 - accuracy: 0.7500 - val_loss: 0.3366 - val_accuracy: 0.8550\n",
      "Epoch 4/50\n",
      "20/20 - 2s - loss: 0.4333 - accuracy: 0.8219 - val_loss: 0.3405 - val_accuracy: 0.8594\n",
      "Epoch 5/50\n",
      "20/20 - 2s - loss: 0.4512 - accuracy: 0.7844 - val_loss: 0.3287 - val_accuracy: 0.8662\n",
      "Epoch 6/50\n",
      "20/20 - 2s - loss: 0.3982 - accuracy: 0.8594 - val_loss: 0.3425 - val_accuracy: 0.8512\n",
      "Epoch 7/50\n",
      "20/20 - 2s - loss: 0.3836 - accuracy: 0.8625 - val_loss: 1.1050 - val_accuracy: 0.6350\n",
      "Epoch 8/50\n",
      "20/20 - 2s - loss: 0.3954 - accuracy: 0.8344 - val_loss: 0.3217 - val_accuracy: 0.8640\n",
      "Epoch 9/50\n",
      "20/20 - 3s - loss: 0.3933 - accuracy: 0.8438 - val_loss: 0.2946 - val_accuracy: 0.8774\n",
      "Epoch 10/50\n",
      "20/20 - 3s - loss: 0.3654 - accuracy: 0.8344 - val_loss: 0.3012 - val_accuracy: 0.8694\n",
      "Epoch 11/50\n",
      "20/20 - 3s - loss: 0.3155 - accuracy: 0.8500 - val_loss: 0.2841 - val_accuracy: 0.8792\n",
      "Epoch 12/50\n",
      "20/20 - 3s - loss: 0.3084 - accuracy: 0.8906 - val_loss: 0.2721 - val_accuracy: 0.8818\n",
      "Epoch 13/50\n",
      "20/20 - 2s - loss: 0.2657 - accuracy: 0.8938 - val_loss: 0.3154 - val_accuracy: 0.8668\n",
      "Epoch 14/50\n",
      "20/20 - 2s - loss: 0.3231 - accuracy: 0.8594 - val_loss: 0.3614 - val_accuracy: 0.8332\n",
      "Epoch 15/50\n",
      "20/20 - 2s - loss: 0.3304 - accuracy: 0.8750 - val_loss: 0.2802 - val_accuracy: 0.8870\n",
      "Epoch 16/50\n",
      "20/20 - 2s - loss: 0.3099 - accuracy: 0.8719 - val_loss: 0.3723 - val_accuracy: 0.8444\n",
      "Epoch 17/50\n",
      "20/20 - 2s - loss: 0.3049 - accuracy: 0.8813 - val_loss: 0.2681 - val_accuracy: 0.8902\n",
      "Epoch 18/50\n",
      "20/20 - 2s - loss: 0.3183 - accuracy: 0.8594 - val_loss: 0.2628 - val_accuracy: 0.8946\n",
      "Epoch 19/50\n",
      "20/20 - 2s - loss: 0.2946 - accuracy: 0.8562 - val_loss: 0.2836 - val_accuracy: 0.8866\n",
      "Epoch 20/50\n",
      "20/20 - 2s - loss: 0.3669 - accuracy: 0.8344 - val_loss: 0.2596 - val_accuracy: 0.8976\n",
      "Epoch 21/50\n",
      "20/20 - 3s - loss: 0.2811 - accuracy: 0.8813 - val_loss: 0.2494 - val_accuracy: 0.8982\n",
      "Epoch 22/50\n",
      "20/20 - 2s - loss: 0.2834 - accuracy: 0.8719 - val_loss: 0.2974 - val_accuracy: 0.8870\n",
      "Epoch 23/50\n",
      "20/20 - 3s - loss: 0.2847 - accuracy: 0.8906 - val_loss: 0.2815 - val_accuracy: 0.8894\n",
      "Epoch 24/50\n",
      "20/20 - 2s - loss: 0.3160 - accuracy: 0.8719 - val_loss: 0.2979 - val_accuracy: 0.8690\n",
      "Epoch 25/50\n",
      "20/20 - 1s - loss: 0.2604 - accuracy: 0.8938 - val_loss: 0.2499 - val_accuracy: 0.8970\n",
      "Epoch 26/50\n",
      "20/20 - 1s - loss: 0.2682 - accuracy: 0.8781 - val_loss: 0.4109 - val_accuracy: 0.8138\n",
      "Epoch 27/50\n",
      "20/20 - 3s - loss: 0.2614 - accuracy: 0.8875 - val_loss: 0.3044 - val_accuracy: 0.8624\n",
      "Epoch 28/50\n",
      "20/20 - 3s - loss: 0.3032 - accuracy: 0.8531 - val_loss: 0.2562 - val_accuracy: 0.8972\n",
      "Epoch 29/50\n",
      "20/20 - 3s - loss: 0.2661 - accuracy: 0.9000 - val_loss: 0.2862 - val_accuracy: 0.8776\n",
      "Epoch 30/50\n",
      "20/20 - 3s - loss: 0.2344 - accuracy: 0.9031 - val_loss: 0.3140 - val_accuracy: 0.8886\n",
      "Epoch 31/50\n",
      "20/20 - 2s - loss: 0.3677 - accuracy: 0.8469 - val_loss: 0.3024 - val_accuracy: 0.8694\n",
      "Epoch 32/50\n",
      "20/20 - 3s - loss: 0.2666 - accuracy: 0.8813 - val_loss: 0.4873 - val_accuracy: 0.8034\n",
      "Epoch 33/50\n",
      "20/20 - 3s - loss: 0.2745 - accuracy: 0.8719 - val_loss: 0.3110 - val_accuracy: 0.8644\n",
      "Epoch 34/50\n",
      "20/20 - 3s - loss: 0.2702 - accuracy: 0.8750 - val_loss: 0.2482 - val_accuracy: 0.8990\n",
      "Epoch 35/50\n",
      "20/20 - 3s - loss: 0.2551 - accuracy: 0.9031 - val_loss: 0.2686 - val_accuracy: 0.8908\n",
      "Epoch 36/50\n",
      "20/20 - 3s - loss: 0.2815 - accuracy: 0.8875 - val_loss: 0.3150 - val_accuracy: 0.8780\n",
      "Epoch 37/50\n",
      "20/20 - 3s - loss: 0.2447 - accuracy: 0.9062 - val_loss: 0.2990 - val_accuracy: 0.8962\n",
      "Epoch 38/50\n",
      "20/20 - 2s - loss: 0.2627 - accuracy: 0.8875 - val_loss: 0.2803 - val_accuracy: 0.8876\n",
      "Epoch 39/50\n",
      "20/20 - 3s - loss: 0.2698 - accuracy: 0.8906 - val_loss: 0.3232 - val_accuracy: 0.8574\n",
      "Epoch 40/50\n",
      "20/20 - 2s - loss: 0.2855 - accuracy: 0.8750 - val_loss: 0.4623 - val_accuracy: 0.8066\n",
      "Epoch 41/50\n",
      "20/20 - 2s - loss: 0.2704 - accuracy: 0.8875 - val_loss: 0.2702 - val_accuracy: 0.8916\n",
      "Epoch 42/50\n",
      "20/20 - 1s - loss: 0.3076 - accuracy: 0.8719 - val_loss: 0.3307 - val_accuracy: 0.8526\n",
      "Epoch 43/50\n",
      "20/20 - 1s - loss: 0.2800 - accuracy: 0.8719 - val_loss: 0.2396 - val_accuracy: 0.9066\n",
      "Epoch 44/50\n",
      "20/20 - 1s - loss: 0.2774 - accuracy: 0.8813 - val_loss: 0.2446 - val_accuracy: 0.9010\n",
      "Epoch 45/50\n",
      "20/20 - 1s - loss: 0.2891 - accuracy: 0.8750 - val_loss: 0.2444 - val_accuracy: 0.9018\n",
      "Epoch 46/50\n",
      "20/20 - 1s - loss: 0.2982 - accuracy: 0.8687 - val_loss: 0.2888 - val_accuracy: 0.8752\n",
      "Epoch 47/50\n",
      "20/20 - 1s - loss: 0.2326 - accuracy: 0.9000 - val_loss: 0.2575 - val_accuracy: 0.8942\n",
      "Epoch 48/50\n",
      "20/20 - 1s - loss: 0.2913 - accuracy: 0.8625 - val_loss: 0.2501 - val_accuracy: 0.8986\n",
      "Epoch 49/50\n",
      "20/20 - 1s - loss: 0.2505 - accuracy: 0.8875 - val_loss: 0.2519 - val_accuracy: 0.8982\n",
      "Epoch 50/50\n",
      "20/20 - 1s - loss: 0.1669 - accuracy: 0.9281 - val_loss: 0.5586 - val_accuracy: 0.8282\n",
      "INFO:tensorflow:Assets written to: bottleneck_fc_model\\assets\n"
     ]
    }
   ],
   "source": [
    "nb_train_samples = 20000\n",
    "nb_validation_samples = 5000\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy','rb'))\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy','rb'))\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
    "\n",
    "    # Building of the small fully-connected model\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Configuration of the learning process\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(train_data,\n",
    "            train_labels, \n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size, \n",
    "            steps_per_epoch = 20, ################\n",
    "            validation_data=(validation_data,validation_labels), \n",
    "            #callbacks=[tensorboard_callback],\n",
    "            verbose=2)\n",
    "    \n",
    "    model.save('bottleneck_fc_model') \n",
    "\n",
    "train_top_model()\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build new model = VGG16 + Top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_5 (Sequential)    (None, 1)                 2130433   \n",
      "=================================================================\n",
      "Total params: 16,845,121\n",
      "Trainable params: 16,845,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = '../keras/examples/vgg16_weights.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "# keras.backend.set_image_dim_ordering('tf')\n",
    "backend.image_data_format()\n",
    "\n",
    "# creation of the base VGG pre-trained model\n",
    "model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height,3))\n",
    "print('Model loaded.')\n",
    "\n",
    "#fetch the previously trained classifier\n",
    "top_model = keras.models.load_model('bottleneck_fc_model')\n",
    "\n",
    "\n",
    "# creation of a real model from vgg\n",
    "new_model = Sequential()\n",
    "for l in model.layers:\n",
    "    new_model.add(l)\n",
    "\n",
    "\n",
    "# concatenation of the base model with the top model\n",
    "new_model.add(top_model)\n",
    "\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "20/20 - 119s - loss: 0.4497 - accuracy: 0.8375 - val_loss: 0.2408 - val_accuracy: 0.9187\n",
      "Epoch 2/10\n",
      "20/20 - 106s - loss: 0.2703 - accuracy: 0.8781 - val_loss: 0.1504 - val_accuracy: 0.9463\n",
      "Epoch 3/10\n",
      "20/20 - 108s - loss: 0.2097 - accuracy: 0.9125 - val_loss: 0.1294 - val_accuracy: 0.9450\n",
      "Epoch 4/10\n",
      "20/20 - 112s - loss: 0.1793 - accuracy: 0.9312 - val_loss: 0.1419 - val_accuracy: 0.9375\n",
      "Epoch 5/10\n",
      "20/20 - 109s - loss: 0.2116 - accuracy: 0.9250 - val_loss: 0.1460 - val_accuracy: 0.9400\n",
      "Epoch 6/10\n",
      "20/20 - 99s - loss: 0.1871 - accuracy: 0.9281 - val_loss: 0.1672 - val_accuracy: 0.9350\n",
      "Epoch 7/10\n",
      "20/20 - 74s - loss: 0.1998 - accuracy: 0.9062 - val_loss: 0.1549 - val_accuracy: 0.9375\n",
      "Epoch 8/10\n",
      "20/20 - 73s - loss: 0.1553 - accuracy: 0.9312 - val_loss: 0.1436 - val_accuracy: 0.9450\n",
      "Epoch 9/10\n",
      "20/20 - 73s - loss: 0.1163 - accuracy: 0.9625 - val_loss: 0.1618 - val_accuracy: 0.9475\n",
      "Epoch 10/10\n",
      "20/20 - 73s - loss: 0.1149 - accuracy: 0.9594 - val_loss: 0.1423 - val_accuracy: 0.9438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 20508), started 11:32:17 ago. (Use '!kill 20508' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b87dd959e7a67c7f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b87dd959e7a67c7f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_dir = 'Lab/data/DogCatChallenge/sampleDeep/train'\n",
    "validation_data_dir = 'Lab/data/DogCatChallenge/sampleDeep/valid'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 10 #50\n",
    "batch_size = 16\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "\n",
    "###################\n",
    "# for layer in new_model.layers[:15]:\n",
    "#    layer.trainable = False\n",
    "###################\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "new_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(learning_rate=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "\n",
    "# fine-tune the model\n",
    "\n",
    "logdir = os.path.join(\"logs1\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_accuracy', verbose=3, save_best_only=True, save_weights_only=False, mode='max', save_freq=1)\n",
    "# early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=3, mode='max')\n",
    "# hist = model.fit_generator(steps_per_epoch=100,generator=traindata, validation_data= testdata, validation_steps=10,epochs=100,callbacks=[checkpoint,early])\n",
    "\n",
    "history = new_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=20, #nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        batch_size = batch_size,\n",
    "        ####### validation_steps=nb_validation_samples // batch_size,\n",
    "        callbacks=[tensorboard_callback], #,checkpoint,early],\n",
    "        verbose=2)\n",
    "\n",
    "# for key in history.history:\n",
    "#     print(key)\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import load_img\n",
    "# from keras.preprocessing.image import img_to_array\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "# from keras.applications.vgg16 import decode_predictions\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# import os\n",
    "# dirname = os.path.dirname(\"__file__\")\n",
    "# img_path = os.path.join(dirname, 'dogs-vs-cats/test1/test1/34.jpg') #dog\n",
    "\n",
    "# # load an image from file\n",
    "# image = load_img(img_path, target_size=(img_width, img_height))\n",
    "# # convert the image pixels to a numpy array\n",
    "# image = img_to_array(image)\n",
    "# # reshape data for the model\n",
    "# image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# # prepare the image for the VGG model\n",
    "# image = preprocess_input(image)\n",
    "# # predict the probability across all output classes\n",
    "# yhat = new_model.predict(image) #[0][0]\n",
    "# # pred = 0 if yhat<0.5 else 1\n",
    "# # print(pred)\n",
    "# label = 'Cat' if yhat == 0 else 'Dog'\n",
    "# print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for the whole test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n",
      "12500/12500 [==============================] - 1238s 99ms/step\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "# test_data_dir = 'Lab/data/DogCatChallenge/sampleDeep/test_main'\n",
    "test_data_dir = 'dogs-vs-cats-redux-kernels-edition/test'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "testgen = test_datagen.flow_from_directory(test_data_dir,\n",
    "                                        target_size=(img_width, img_height),\n",
    "                                        class_mode=None,\n",
    "                                        # classes=class_subset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "\n",
    "# from tf.keras.models import load_model\n",
    "\n",
    "# final_model = load_model('saved_model/my_model')\n",
    "\n",
    "vgg_preds = new_model.predict(testgen, verbose=1)\n",
    "L = len(vgg_preds)\n",
    "vgg_pred_list = vgg_preds.tolist()\n",
    "full_pred = [vgg_pred_list[i][0] for i in range(L)]\n",
    "print(len(full_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "submission = pd.read_csv(\"dogs-vs-cats-redux-kernels-edition/sample_submission.csv\")\n",
    "submission['label'] = full_pred\n",
    "submission.to_csv(\"full_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = [0 if x<0.5 else 1 for x in full_pred]\n",
    "submission = pd.read_csv(\"dogs-vs-cats-redux-kernels-edition/sample_submission.csv\")\n",
    "submission['label'] = final_pred\n",
    "submission.to_csv(\"full_sub1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging... (prediction on new labeled images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 1 classes.\n",
      "2000/2000 [==============================] - 178s 89ms/step\n"
     ]
    }
   ],
   "source": [
    "# test_data_dir = 'Lab/data/DogCatChallenge/sampleDeep/test_main'\n",
    "test_data_dir = 'dogs-vs-cats-redux-kernels-edition/test_try'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "testgen = test_datagen.flow_from_directory(test_data_dir,\n",
    "                                        target_size=(img_width, img_height),\n",
    "                                        class_mode=None,\n",
    "                                        # classes=class_subset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)\n",
    "\n",
    "# from tf.keras.models import load_model\n",
    "\n",
    "# final_model = load_model('saved_model/my_model')\n",
    "\n",
    "vgg_preds_try = new_model.predict(testgen, verbose=1)\n",
    "L = len(vgg_preds_try)\n",
    "vgg_pred_list = vgg_preds_try.tolist()\n",
    "full_pred = [vgg_pred_list[i][0] for i in range(L)]\n",
    "final_pred = [0 if x<0.5 else 1 for x in full_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1.1743239946089754\n",
      "0.966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "labels = [0]*1000 + [1]*1000\n",
    "print(len(final_pred))\n",
    "print(log_loss(labels,final_pred))\n",
    "print(accuracy_score(labels,final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c25d70a1238092e1b4e9fc063355643dd3a3802177daea5be807482ee58a503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dogs_cats')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
